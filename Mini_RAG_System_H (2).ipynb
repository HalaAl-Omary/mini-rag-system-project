{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Homework: Build and Test a Mini RAG System from Scratch ğŸ§ \n",
        "\n",
        "> **ğŸ¯ Today's Goal**: Combine the knowledge from the first three lessons (Embeddings, Retrieval, Generation) to build a functional Retrieval-Augmented Generation (RAG) system from scratch. Then, test it with a self-assessment!"
      ],
      "metadata": {
        "id": "Fg7IRyVjzb74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers torch"
      ],
      "metadata": {
        "id": "NCUjET-gzbee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ Part 1: The Retriever - Finding the Right Knowledge\n",
        "\n",
        "First, we'll set up our Retriever. Its job is to take a question and find the most relevant piece of text from our knowledge base.\n",
        "\n",
        "1.  **Load the Embedding Model** (`all-MiniLM-L6-v2`)\n",
        "2.  **Create our Knowledge Base**\n",
        "3.  **Encode Everything into Embeddings**\n",
        "4.  **Calculate Similarity** to find the best match"
      ],
      "metadata": {
        "id": "99P0U4tJzjq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "\n",
        "# 1. Load our embedding model\n",
        "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 2. Create a simple knowledge base\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.\",\n",
        "    \"The Amazon rainforest is the world's largest tropical rainforest, known for its incredible biodiversity.\",\n",
        "    \"Mount Everest is the highest mountain on Earth, located in the Himalayas.\",\n",
        "    \"The Great Wall of China is a series of fortifications stretching over 13,000 miles.\",\n",
        "    \"Photosynthesis is the process used by plants to convert light energy into chemical energy.\"\n",
        "]\n",
        "\n",
        "# 3. Encode our knowledge base into embeddings\n",
        "knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "print(f\"âœ… Retriever model loaded and knowledge base encoded with {len(knowledge_base)} documents.\")"
      ],
      "metadata": {
        "id": "CJHpc394zkYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœï¸ Part 2: The Generator - Extracting the Answer\n",
        "\n",
        "Now we set up our Generator. This model will take the question and the context found by the retriever and extract the exact answer from it."
      ],
      "metadata": {
        "id": "Plr7LaDsznpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our question-answering (generator) model\n",
        "generator = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "print(\"âœ… Generator (QA) model loaded.\")"
      ],
      "metadata": {
        "id": "sCHCTi1xzpto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸš€ Part 3: Testing our RAG System\n",
        "\n",
        "Time to put it all together! The function below will simulate a full RAG pipeline and grade itself against a predefined set of questions and answers.\n",
        "\n",
        "It will test two key things:\n",
        "1.  **Retrieval Accuracy**: Did we find the right document?\n",
        "2.  **Generation Accuracy**: Did we extract the correct answer from that document?"
      ],
      "metadata": {
        "id": "kJMFmEjgzsIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_assessment():\n",
        "    \"\"\"Runs a self-assessment of the RAG pipeline with multiple questions.\"\"\"\n",
        "\n",
        "    # Define our questions, expected context keywords, and expected answers\n",
        "    test_questions = [\n",
        "        {\n",
        "            \"question\": \"What is the highest mountain?\",\n",
        "            \"expected_keyword\": \"Everest\",\n",
        "            \"expected_answer\": \"Mount Everest\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Which city is home to the Louvre museum?\",\n",
        "            \"expected_keyword\": \"France\",\n",
        "            \"expected_answer\": \"Paris\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What process do plants use for energy?\",\n",
        "            \"expected_keyword\": \"Photosynthesis\",\n",
        "            \"expected_answer\": \"Photosynthesis\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2 # 2 points per question (1 for retrieval, 1 for generation)\n",
        "\n",
        "    print(\"--- ğŸš€ Starting RAG System Assessment ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "\n",
        "        # --- 1. Retrieval Step ---\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)[0]\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base[top_result_index]\n",
        "\n",
        "        print(f\"ğŸ”  Retrieved Context: '{retrieved_context}'\")\n",
        "\n",
        "        # Check if the retrieval was correct\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"âœ…  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "\n",
        "        # --- 2. Generation Step ---\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "\n",
        "        print(f\"âœï¸  Generated Answer: '{generated_answer}'\")\n",
        "\n",
        "        # Check if the generation was correct\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"âœ…  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    # --- Final Score ---\n",
        "    print(f\"\\n--- ğŸ Assessment Complete ---\")\n",
        "    print(f\"ğŸ¯ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"ğŸ‰ğŸ‰ğŸ‰ Perfect! Your RAG system is working as expected!\")\n",
        "    elif score >= total / 2:\n",
        "        print(\"ğŸ‘ Good job! The system is mostly correct.\")\n",
        "    else:\n",
        "        print(\"ğŸ”§ The system ran into some issues. Review the steps and check the logic.\")\n",
        "\n",
        "# Run the assessment!\n",
        "run_rag_assessment()"
      ],
      "metadata": {
        "id": "OvP6XETQztxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  STUDENT TASKS ğŸ§‘â€ğŸ’»\n",
        "\n",
        "Now it's your turn to be the AI engineer. Your tasks are to run, analyze, and extend the RAG system you've just built."
      ],
      "metadata": {
        "id": "bmsraUmd0ruq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Execute and Understand\n",
        "\n",
        "Your first task is to simply run all the cells above and carefully read the output of the final self-assessment.\n",
        "\n",
        "* **Observe the Score:** Did the system get a perfect score (6/6)?\n",
        "* **Analyze Each Step:** For each question, look at the \"Retrieved Context\" and the \"Generated Answer.\"\n",
        "    * Did the retriever find the correct piece of knowledge?\n",
        "    * Did the generator extract the right answer from that context?"
      ],
      "metadata": {
        "id": "RT5z7ZoE0trr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2 (Challenge): Add a New Question\n",
        "\n",
        "Your second task is to test the system with a new question about the **existing knowledge**.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Copy the code from the cell below. It's the same assessment function as before, but with a new test question added.\n",
        "2.  Run the cell and see if the system can answer correctly. The score should now be out of 8."
      ],
      "metadata": {
        "id": "25iQeOoo6jTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Add a new question to the assessment function\n",
        "\n",
        "def run_rag_assessment_task_2():\n",
        "    test_questions = [\n",
        "        {\n",
        "        \"question\": \"Which city is the capital of France?\",\n",
        "        \"expected_keyword\": \"France\",\n",
        "        \"expected_answer\": \"Paris\"\n",
        "        },\n",
        "        {\n",
        "        \"question\": \"Where is the Amazon rainforest located?\",\n",
        "        \"expected_keyword\": \"Amazon rainforest\",\n",
        "        \"expected_answer\": \"world's largest tropical rainforest\"\n",
        "        },\n",
        "        {\n",
        "        \"question\": \"What is the Great Wall of China known for?\",\n",
        "        \"expected_keyword\": \"Great Wall of China\",\n",
        "        \"expected_answer\": \"fortifications\"\n",
        "        },\n",
        "        {\n",
        "        \"question\": \"How do plants produce energy from sunlight?\",\n",
        "        \"expected_keyword\": \"Photosynthesis\",\n",
        "        \"expected_answer\": \"Photosynthesis\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2\n",
        "\n",
        "    print(\"--- ğŸš€ Starting RAG System Assessment (Task 2) ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)[0]\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base[top_result_index]\n",
        "        print(f\"ğŸ”  Retrieved Context: '{retrieved_context}'\")\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"âœ…  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "        print(f\"âœï¸  Generated Answer: '{generated_answer}'\")\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"âœ…  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    print(f\"\\n--- ğŸ Assessment Complete ---\")\n",
        "    print(f\"ğŸ¯ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"ğŸ‰ğŸ‰ğŸ‰ Perfect! Your RAG system handled the new question!\")\n",
        "\n",
        "# Run the updated assessment\n",
        "run_rag_assessment_task_2()"
      ],
      "metadata": {
        "id": "6TKb4uBO6pEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3 (Advanced Challenge): Add New Knowledge & Test It\n",
        "\n",
        "Your final and most important task is to **expand the RAG system's knowledge base** and then test it.\n",
        "\n",
        "**Instructions:**\n",
        "1.  **Add a new fact** to the `knowledge_base` in the code cell below.\n",
        "2.  **You must re-run this cell** to update the `knowledge_embeddings`! The system won't know about the new fact until you do.\n",
        "3.  Finally, run the last code cell, which has a new test question about the knowledge you just added."
      ],
      "metadata": {
        "id": "JNHQuccw7Duu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3, Step 1: Add a new sentence to the knowledge base\n",
        "\n",
        "knowledge_base_task_3 = [\n",
        "    \"The Moon is the Earth's only natural satellite and affects tides.\"\n",
        "    \"The fastest land animal is the cheetah, capable of speeds up to 75 mph.\"\n",
        "    \"Mercury is the closest planet to the Sun and has no substantial atmosphere.\"\n",
        "\n",
        "]\n",
        "\n",
        "# Re-encode the updated knowledge base\n",
        "knowledge_embeddings_task_3 = retriever_model.encode(knowledge_base_task_3, convert_to_tensor=True)\n",
        "\n",
        "print(f\"âœ… Knowledge base updated and re-encoded with {len(knowledge_base_task_3)} documents.\")"
      ],
      "metadata": {
        "id": "4ijwpN1W7VyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3, Step 2: Test your newly added knowledge\n",
        "\n",
        "def run_rag_assessment_task_3():\n",
        "    test_questions = [\n",
        "        # --- NEW QUESTION FOR YOUR NEW KNOWLEDGE ---\n",
        "        {\n",
        "            \"question\": \"What is Earth's only natural satellite?\",\n",
        "            \"expected_keyword\": \"Moon\",\n",
        "            \"expected_answer\": \"Moon\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2\n",
        "\n",
        "    print(\"--- ğŸš€ Starting RAG System Assessment (Task 3) ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "        # Use the updated embeddings from Task 3\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings_task_3)[0]\n",
        "        # Use the updated knowledge base from Task 3\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base_task_3[top_result_index]\n",
        "        print(f\"ğŸ”  Retrieved Context: '{retrieved_context}'\")\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"âœ…  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "        print(f\"âœï¸  Generated Answer: '{generated_answer}'\")\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"âœ…  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    print(f\"\\n--- ğŸ Assessment Complete ---\")\n",
        "    print(f\"ğŸ¯ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"ğŸ†ğŸ†ğŸ† Success! You have successfully extended the knowledge of your RAG system!\")\n",
        "\n",
        "# Run the final assessment\n",
        "run_rag_assessment_task_3()"
      ],
      "metadata": {
        "id": "-tgQJ13P7pCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Streamlit and pyngrok\n",
        "!pip install streamlit -q\n",
        "!pip install pyngrok -q\n"
      ],
      "metadata": {
        "id": "c9N5DVs0SC0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# ------------------------------\n",
        "# Load models\n",
        "# ------------------------------\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    retriever = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    generator = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "    return retriever, generator\n",
        "\n",
        "retriever_model, generator = load_models()\n",
        "\n",
        "# Knowledge base\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.\",\n",
        "    \"The Amazon rainforest is the world's largest tropical rainforest, known for its incredible biodiversity.\",\n",
        "    \"Mount Everest is the highest mountain on Earth, located in the Himalayas.\",\n",
        "    \"The Great Wall of China is a series of fortifications stretching over 13,000 miles.\",\n",
        "    \"Photosynthesis is the process used by plants to convert light energy into chemical energy.\"\n",
        "]\n",
        "knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "# ------------------------------\n",
        "# Streamlit UI\n",
        "# ------------------------------\n",
        "st.title(\"ğŸ§  Mini RAG System Demo\")\n",
        "st.write(\"Ask a question and the system will find the most relevant context and generate an answer!\")\n",
        "\n",
        "question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "if st.button(\"Get Answer\") and question:\n",
        "    question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "    cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)[0]\n",
        "    top_result_index = torch.argmax(cos_scores)\n",
        "    retrieved_context = knowledge_base[top_result_index]\n",
        "\n",
        "    st.subheader(\"ğŸ” Retrieved Context\")\n",
        "    st.write(retrieved_context)\n",
        "\n",
        "    qa_result = generator(question=question, context=retrieved_context)\n",
        "    st.subheader(\"âœï¸ Generated Answer\")\n",
        "    st.success(qa_result['answer'])\n",
        "else:\n",
        "    st.info(\"Enter a question and click 'Get Answer'.\")\n"
      ],
      "metadata": {
        "id": "7nJVSqkkP4a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Ø¶Ø¹ ØªÙˆÙƒÙ† ngrok Ù‡Ù†Ø§ (Ø§Ø³ØªØ¨Ø¯Ù„ÙŠ Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¹Ø¯ Ù…Ø§ ØªÙ„ØºÙŠ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©)\n",
        "ngrok.set_auth_token(\"33NNp4yunF3sa59CtjWFeS7oiO0_2Fk22thZL69xzVupzp1YZ\")\n",
        "\n",
        "# Ø§ÙØªØ­ Ù†ÙÙ‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙˆØ±Øª 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Ngrok tunnel:\", public_url.public_url)\n",
        "\n",
        "# Ø´ØºÙ‘Ù„ streamlit ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©\n",
        "# Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØªÙÙˆØ¬Ù‘Ù‡ Ø¥Ù„Ù‰ /dev/null Ù„ØªÙØ§Ø¯ÙŠ ÙÙˆØ¶Ù‰ logs ÙÙŠ Ø§Ù„Ø®Ù„ÙŠØ©\n",
        "os.system(\"streamlit run app.py --server.port 8501 &>/dev/null &\")\n",
        "\n",
        "# Ø§Ù†ØªØ¸Ø± Ø´ÙˆÙŠØ© Ù„ÙŠÙ† ÙŠØ¨Ø¯Ø§ Ø§Ù„Ø³ÙŠØ±ÙØ±\n",
        "time.sleep(5)\n",
        "print(\"Open the public URL above in your browser.\")\n"
      ],
      "metadata": {
        "id": "8RExy20VTOUs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}